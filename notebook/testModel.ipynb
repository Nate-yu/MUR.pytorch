{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-14T12:09:27.348691Z",
     "start_time": "2024-05-14T12:09:26.995857Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# 使用预训练的ResNet50模型\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# 输入图像（batch_size, channels, height, width）\n",
    "input_image = torch.randn(1, 3, 224, 224)\n",
    "# print(input_image)\n",
    "\n",
    "# 获取模型的最后一层全连接层的输出（特征向量）\n",
    "with torch.no_grad():\n",
    "    features = model(input_image)\n",
    "\n",
    "print(features.shape)  # 输出: torch.Size([1, 1000])\n",
    "# 在这个例子中，特征向量的长度是1000，因为ResNet50的最后一层有1000个输出单元，对应于1000个分类标签"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T12:15:11.892678Z",
     "start_time": "2024-05-14T12:15:11.862179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 输出第一个特征图\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 输出第二个特征图\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n",
    "print(model)\n",
    "\n",
    "# 输入图像（batch_size, channels, height, width）\n",
    "input_image = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# 获取卷积层输出的特征图\n",
    "with torch.no_grad():\n",
    "    feature_map = model(input_image)\n",
    "\n",
    "print(feature_map.shape)  # 输出: torch.Size([1, 32, 8, 8])"
   ],
   "id": "e540da86900d7e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "torch.Size([1, 32, 8, 8])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "class ExternalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model,S=64):\n",
    "        super().__init__()\n",
    "        self.mk=nn.Linear(d_model,S,bias=False)\n",
    "        self.mv=nn.Linear(S,d_model,bias=False)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, queries):\n",
    "        attn=self.mk(queries) #bs,n,S\n",
    "        attn=self.softmax(attn) #bs,n,S\n",
    "        attn=attn/torch.sum(attn,dim=2,keepdim=True) #bs,n,S\n",
    "        out=self.mv(attn) #bs,n,d_model\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input=torch.randn(50,49,512)\n",
    "    ea = ExternalAttention(d_model=512,S=8)\n",
    "    output=ea(input)\n",
    "    print(output.shape)\n",
    "\n"
   ],
   "id": "317cd79c15a826e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T08:13:42.999464Z",
     "start_time": "2024-05-16T08:13:40.990533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.attention_modules.self_attention import AttentionModule\n",
    "model = AttentionModule(feature_size=512,text_feature_size=512, num_heads=8)\n",
    "print(model)"
   ],
   "id": "298353be4be42a3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionModule(\n",
      "  (gam_attention): GAM_Attention(\n",
      "    (channel_attention): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=128, out_features=512, bias=True)\n",
      "    )\n",
      "    (spatial_attention): Sequential(\n",
      "      (0): Conv2d(512, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (self_att_generator): SelfAttentionMap(\n",
      "    (W_k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (W_q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (softmax): Softmax(dim=2)\n",
      "  )\n",
      "  (global_att_generator): GlobalCrossAttentionMap(\n",
      "    (W_t): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (normalize): Softmax(dim=1)\n",
      "  )\n",
      "  (merge): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (W_v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (W_r): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:29:39.571722Z",
     "start_time": "2024-05-19T13:29:37.136294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaModel, RobertaTokenizer\n"
   ],
   "id": "8587c43e147b1c4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:29:43.262680Z",
     "start_time": "2024-05-19T13:29:43.253593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 一个简单的RoBERTa模型\n",
    "class SimpleRoBERTa(nn.Module):\n",
    "    def __init__(self, pretrained_model_name='roberta-base'):\n",
    "        super(SimpleRoBERTa, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids, attention_mask)\n",
    "        return outputs\n"
   ],
   "id": "6ae089f15479940c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:32:04.044065Z",
     "start_time": "2024-05-19T13:31:53.890792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义一个用于文本编码的RoBERTaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n"
   ],
   "id": "865bd130e54d5882",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:34:35.756025Z",
     "start_time": "2024-05-19T13:34:15.492609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 现在，我们可以使用这个简单的RoBERTa模型来对文本进行编码了：\n",
    "# 输入文本\n",
    "text = \"I love natural language processing!\"\n",
    "\n",
    "# 使用tokenizer将文本转换为模型所需的输入格式\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=True, return_tensors='pt')\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "# 实例化和加载模型\n",
    "model = SimpleRoBERTa()\n",
    "outputs = model(input_ids, attention_mask)\n",
    "\n",
    "# 输出编码结果\n",
    "print(outputs.size())\n"
   ],
   "id": "cef064c7453eb105",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 14\u001B[0m\n\u001B[0;32m     11\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(input_ids, attention_mask)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# 输出编码结果\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43moutputs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m())\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'size'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:37:01.456816Z",
     "start_time": "2024-05-19T13:36:31.088981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "# 加载 RoBERTa 模型和分词器\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# 输入文本\n",
    "text = \"Hello, how are you?\"\n",
    "\n",
    "# 使用分词器对文本进行编码\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 传递输入到模型中进行推断\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# 打印输出张量的形状\n",
    "print(\"Last hidden state shape:\", outputs.last_hidden_state.shape)\n",
    "print(\"Pooler output shape:\", outputs.pooler_output.shape)\n"
   ],
   "id": "b0aee913a168ae3e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last hidden state shape: torch.Size([1, 8, 768])\n",
      "Pooler output shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T07:24:02.019165Z",
     "start_time": "2024-06-03T07:24:00.643408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.models import resnet18, resnet50\n",
    "model = resnet50(pretrained=True)\n",
    "print(model)"
   ],
   "id": "9587fe7579d66c50",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\IDE\\Miniconda\\envs\\cv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\IDE\\Miniconda\\envs\\cv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class eca_layer(nn.Module):\n",
    "    \"\"\"Constructs a ECA module.\n",
    "\n",
    "    Args:\n",
    "        channel: Number of channels of the input feature map\n",
    "        k_size: Adaptive selection of kernel size\n",
    "    \"\"\"\n",
    "    def __init__(self, channel, k_size=3):\n",
    "        super(eca_layer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # feature descriptor on the global spatial information\n",
    "        y = self.avg_pool(x)\n",
    "\n",
    "        # Two different branches of ECA module\n",
    "        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
    "\n",
    "        # Multi-scale information fusion\n",
    "        y = self.sigmoid(y)\n",
    "\n",
    "        return x * y.expand_as(x)\n",
    "        "
   ],
   "id": "cdda07314788335"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "# import torch.utils.model_zoo as model_zoo\n",
    "from .eca_module import eca_layer\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class ECABasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, k_size=3):\n",
    "        super(ECABasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.eca = eca_layer(planes, k_size)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.eca(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ECABottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, k_size=3):\n",
    "        super(ECABottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.eca = eca_layer(planes * 4, k_size)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.eca(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, k_size=[3, 3, 3, 3]):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], int(k_size[0]))\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], int(k_size[1]), stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], int(k_size[2]), stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], int(k_size[3]), stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, k_size, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, k_size))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, k_size=k_size))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def eca_resnet18(k_size=[3, 3, 3, 3], num_classes=1_000, pretrained=False):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "\n",
    "    Args:\n",
    "        k_size: Adaptive selection of kernel size\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        num_classes:The classes of classification\n",
    "    \"\"\"\n",
    "    model = ResNet(ECABasicBlock, [2, 2, 2, 2], num_classes=num_classes, k_size=k_size)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def eca_resnet50(k_size=[3, 3, 3, 3], num_classes=1000, pretrained=False):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "\n",
    "    Args:\n",
    "        k_size: Adaptive selection of kernel size\n",
    "        num_classes:The classes of classification\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    print(\"Constructing eca_resnet50......\")\n",
    "    model = ResNet(ECABottleneck, [3, 4, 6, 3], num_classes=num_classes, k_size=k_size)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model"
   ],
   "id": "9e3866f443f9107a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T14:34:59.749306Z",
     "start_time": "2024-06-05T14:34:56.788310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.attention_modules.self_attention import AttentionModule, SelfAttentionMap, GlobalCrossAttentionMap\n",
    "model = AttentionModule(512,512,2)\n",
    "print(model)"
   ],
   "id": "d2ab4b1d43e6f6f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionModule(\n",
      "  (gam_attention): GAM_Attention(\n",
      "    (channel_attention): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=128, out_features=512, bias=True)\n",
      "    )\n",
      "    (spatial_attention): Sequential(\n",
      "      (0): Conv2d(512, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (self_att_generator): SelfAttentionMap(\n",
      "    (W_k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (W_q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (softmax): Softmax(dim=2)\n",
      "  )\n",
      "  (global_att_generator): GlobalCrossAttentionMap(\n",
      "    (W_t): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (normalize): Softmax(dim=1)\n",
      "  )\n",
      "  (merge): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (W_v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (W_r): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "74cfa71396068c04"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
