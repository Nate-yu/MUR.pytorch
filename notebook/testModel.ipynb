{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# 使用预训练的ResNet50模型\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# 输入图像（batch_size, channels, height, width）\n",
    "input_image = torch.randn(1, 3, 224, 224)\n",
    "# print(input_image)\n",
    "\n",
    "# 获取模型的最后一层全连接层的输出（特征向量）\n",
    "with torch.no_grad():\n",
    "    features = model(input_image)\n",
    "\n",
    "print(features.shape)  # 输出: torch.Size([1, 1000])\n",
    "# 在这个例子中，特征向量的长度是1000，因为ResNet50的最后一层有1000个输出单元，对应于1000个分类标签"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 输出第一个特征图\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 输出第二个特征图\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n",
    "print(model)\n",
    "\n",
    "# 输入图像（batch_size, channels, height, width）\n",
    "input_image = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# 获取卷积层输出的特征图\n",
    "with torch.no_grad():\n",
    "    feature_map = model(input_image)\n",
    "\n",
    "print(feature_map.shape)  # 输出: torch.Size([1, 32, 8, 8])"
   ],
   "id": "e540da86900d7e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "class ExternalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model,S=64):\n",
    "        super().__init__()\n",
    "        self.mk=nn.Linear(d_model,S,bias=False)\n",
    "        self.mv=nn.Linear(S,d_model,bias=False)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, queries):\n",
    "        attn=self.mk(queries) #bs,n,S\n",
    "        attn=self.softmax(attn) #bs,n,S\n",
    "        attn=attn/torch.sum(attn,dim=2,keepdim=True) #bs,n,S\n",
    "        out=self.mv(attn) #bs,n,d_model\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input=torch.randn(50,49,512)\n",
    "    ea = ExternalAttention(d_model=512,S=8)\n",
    "    output=ea(input)\n",
    "    print(output.shape)\n",
    "\n"
   ],
   "id": "317cd79c15a826e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from models.attention_modules.self_attention import AttentionModule\n",
    "model = AttentionModule(feature_size=512,text_feature_size=512, num_heads=8)\n",
    "print(model)"
   ],
   "id": "298353be4be42a3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaModel, RobertaTokenizer\n"
   ],
   "id": "8587c43e147b1c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 一个简单的RoBERTa模型\n",
    "class SimpleRoBERTa(nn.Module):\n",
    "    def __init__(self, pretrained_model_name='roberta-base'):\n",
    "        super(SimpleRoBERTa, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids, attention_mask)\n",
    "        return outputs\n"
   ],
   "id": "6ae089f15479940c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 定义一个用于文本编码的RoBERTaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n"
   ],
   "id": "865bd130e54d5882",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 现在，我们可以使用这个简单的RoBERTa模型来对文本进行编码了：\n",
    "# 输入文本\n",
    "text = \"I love natural language processing!\"\n",
    "\n",
    "# 使用tokenizer将文本转换为模型所需的输入格式\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=True, return_tensors='pt')\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "# 实例化和加载模型\n",
    "model = SimpleRoBERTa()\n",
    "outputs = model(input_ids, attention_mask)\n",
    "\n",
    "# 输出编码结果\n",
    "print(outputs.size())\n"
   ],
   "id": "cef064c7453eb105",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "# 加载 RoBERTa 模型和分词器\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# 输入文本\n",
    "text = \"Hello, how are you?\"\n",
    "\n",
    "# 使用分词器对文本进行编码\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 传递输入到模型中进行推断\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# 打印输出张量的形状\n",
    "print(\"Last hidden state shape:\", outputs.last_hidden_state.shape)\n",
    "print(\"Pooler output shape:\", outputs.pooler_output.shape)\n"
   ],
   "id": "b0aee913a168ae3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchvision.models import resnet18, resnet50\n",
    "model = resnet50(pretrained=True)\n",
    "print(model)"
   ],
   "id": "9587fe7579d66c50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class eca_layer(nn.Module):\n",
    "    \"\"\"Constructs a ECA module.\n",
    "\n",
    "    Args:\n",
    "        channel: Number of channels of the input feature map\n",
    "        k_size: Adaptive selection of kernel size\n",
    "    \"\"\"\n",
    "    def __init__(self, channel, k_size=3):\n",
    "        super(eca_layer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # feature descriptor on the global spatial information\n",
    "        y = self.avg_pool(x)\n",
    "\n",
    "        # Two different branches of ECA module\n",
    "        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
    "\n",
    "        # Multi-scale information fusion\n",
    "        y = self.sigmoid(y)\n",
    "\n",
    "        return x * y.expand_as(x)\n",
    "        "
   ],
   "id": "cdda07314788335",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "# import torch.utils.model_zoo as model_zoo\n",
    "from .eca_module import eca_layer\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class ECABasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, k_size=3):\n",
    "        super(ECABasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.eca = eca_layer(planes, k_size)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.eca(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ECABottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, k_size=3):\n",
    "        super(ECABottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.eca = eca_layer(planes * 4, k_size)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.eca(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, k_size=[3, 3, 3, 3]):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], int(k_size[0]))\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], int(k_size[1]), stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], int(k_size[2]), stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], int(k_size[3]), stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, k_size, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, k_size))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, k_size=k_size))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def eca_resnet18(k_size=[3, 3, 3, 3], num_classes=1_000, pretrained=False):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "\n",
    "    Args:\n",
    "        k_size: Adaptive selection of kernel size\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        num_classes:The classes of classification\n",
    "    \"\"\"\n",
    "    model = ResNet(ECABasicBlock, [2, 2, 2, 2], num_classes=num_classes, k_size=k_size)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def eca_resnet50(k_size=[3, 3, 3, 3], num_classes=1000, pretrained=False):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "\n",
    "    Args:\n",
    "        k_size: Adaptive selection of kernel size\n",
    "        num_classes:The classes of classification\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    print(\"Constructing eca_resnet50......\")\n",
    "    model = ResNet(ECABottleneck, [3, 4, 6, 3], num_classes=num_classes, k_size=k_size)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model"
   ],
   "id": "9e3866f443f9107a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from models.attention_modules.self_attention import AttentionModule, SelfAttentionMap, GlobalCrossAttentionMap\n",
    "model = AttentionModule(512,512,2)\n",
    "print(model)"
   ],
   "id": "d2ab4b1d43e6f6f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "compositor_object1=1\n",
    "compositor_object2=13\n",
    "\n",
    "text_encoder_object=2\n",
    "lower_img_encoder_object=2\n",
    "upper_img_encoder_object=2\n",
    "\n",
    "compositors = {\n",
    "    'compositor1': compositor_object1,\n",
    "    'compositor2': compositor_object2,\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'text_encoder': text_encoder_object,\n",
    "    'lower_image_encoder': lower_img_encoder_object,\n",
    "    'upper_image_encoder': upper_img_encoder_object,\n",
    "}\n",
    "models"
   ],
   "id": "74cfa71396068c04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models.update(compositors)\n",
    "print(models)"
   ],
   "id": "254ea067b0ec86a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from models.compositors.global_style_models import GlobalStyleTransformer2\n",
    "from models.compositors.transformers import DisentangledTransformer\n",
    "\n",
    "\n",
    "def global_styler_factory(code, feature_size, text_feature_size):\n",
    "    if code == GlobalStyleTransformer2.code():\n",
    "        return GlobalStyleTransformer2(feature_size, text_feature_size)\n",
    "    else:\n",
    "        raise ValueError(\"{} not exists\".format(code))\n",
    "\n",
    "\n",
    "def transformer_factory(feature_sizes, configs):\n",
    "    text_feature_size = feature_sizes['text_feature_size']\n",
    "    num_heads = configs['num_heads']\n",
    "\n",
    "    global_styler_code = configs['global_styler']\n",
    "    global_styler = global_styler_factory(global_styler_code, feature_sizes['layer4'], text_feature_size)\n",
    "    return {'layer4': DisentangledTransformer(feature_sizes['layer4'], text_feature_size, num_heads=num_heads,\n",
    "                                              global_styler=global_styler)}\n"
   ],
   "id": "45ae897eca8b64f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T01:57:13.307735Z",
     "start_time": "2024-07-06T01:57:13.286680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.compositors import transformer_factory\n",
    "from models.image_encoders import image_encoder_factory\n",
    "from models.text_encoders import text_encoder_factory\n",
    "from language import vocabulary_factory\n",
    "from data.shoes import ShoesDataset\n",
    "DEFAULT_VOCAB_PATHS = {\n",
    "    **dict.fromkeys(ShoesDataset.all_codes(), ShoesDataset.vocab_path()),\n",
    "}\n",
    "DEFAULT_VOCAB_PATHS"
   ],
   "id": "503a4bbac005a3b8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shoes': './data/shoes/shoes_vocab.pkl'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T02:26:02.181983Z",
     "start_time": "2024-07-06T02:26:02.173864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "configs = {\n",
    "    'alpha_scale': 1,\n",
    "    'augmenter': 'normal_gaussian',\n",
    "    'batch_size': 24,\n",
    "    'beta_scale': 1,\n",
    "    'checkpoint_path': './ckpt',\n",
    "    'compositor': 'transformer',\n",
    "    'config_path': 'configs/shoes_config.json',\n",
    "    'dataset': 'shoes',\n",
    "    'decay_step': 35,\n",
    "    'decay_step_second': 42,\n",
    "    'device_idx': '0',\n",
    "    'epoch': 1,\n",
    "    'evaluator': 'simple',\n",
    "    'experiment_description': 'bert',\n",
    "    'experiment_dir': 'experiments',\n",
    "    'export_root': 'experiments\\\\bert_2024-07-06_0',\n",
    "    'feature_size': 512,\n",
    "    'gamma': 0.1,\n",
    "    'gamma_scale': 1,\n",
    "    'global_styler': 'global2',\n",
    "    'heads': 12,\n",
    "    'image_encoder': 'resnet50_layer4',\n",
    "    'img_size': 224,\n",
    "    'layers': 12,\n",
    "    'lr': 0.0001,\n",
    "    'lr_scheduler': 'MultiStepWithWarmup',\n",
    "    'lstm_hidden_size': 512,\n",
    "    'margin': 12,\n",
    "    'metric_loss': 'batch_based_aleatoric_loss',\n",
    "    'momentum': 0.9,\n",
    "    'norm_scale': 4,\n",
    "    'num_gpu': 1,\n",
    "    'num_heads': 8,\n",
    "    'num_workers': 0,\n",
    "    'optimizer': 'RAdam',\n",
    "    'output_dim': 512,\n",
    "    'patch_size': 16,\n",
    "    'random_seed': 13877,\n",
    "    'selector': 'all',\n",
    "    'shuffle': True,\n",
    "    'stride': True,\n",
    "    'stride_size': 16,\n",
    "    'text_encoder': 'roberta',\n",
    "    'text_feature_size': 512,\n",
    "    'topk': '1,5,10,50',\n",
    "    'trainer': 'tirg',\n",
    "    'use_transform': True,\n",
    "    'vocab_path': '../data/shoes/shoes_vocab.pkl',\n",
    "    'vocab_threshold': 0,\n",
    "    'wandb_account_name': 'yubin06',\n",
    "    'wandb_project_name': 'UR',\n",
    "    'warmup_iters': 5,\n",
    "    'weight_decay': 5e-05,\n",
    "    'width': 768,\n",
    "    'word_embedding_size': 512\n",
    "}\n",
    "vocabulary = vocabulary_factory(config={\n",
    "    'vocab_path': configs['vocab_path'] if configs['vocab_path'] else DEFAULT_VOCAB_PATHS[configs['dataset']],\n",
    "    'vocab_threshold': configs['vocab_threshold']\n",
    "})\n",
    "print(vocabulary)"
   ],
   "id": "d0f57217d7aa3531",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<language.vocabulary.SimpleVocabulary object at 0x00000220D8EFF370>\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T02:26:16.700725Z",
     "start_time": "2024-07-06T02:26:02.717731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_encoder, text_fc = text_encoder_factory(vocabulary, config=configs)\n",
    "lower_img_encoder, upper_img_encoder = image_encoder_factory(config=configs)\n",
    "layer_shapes = lower_img_encoder.layer_shapes()\n",
    "compositors = transformer_factory({'layer4': layer_shapes['layer4'],\n",
    "                                       'image_feature_size': upper_img_encoder.feature_size,\n",
    "                                       'text_feature_size': text_encoder.feature_size}, configs=configs)"
   ],
   "id": "1d33da9b36f5cf94",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T02:26:19.166149Z",
     "start_time": "2024-07-06T02:26:19.156494Z"
    }
   },
   "cell_type": "code",
   "source": "print(compositors)",
   "id": "9456e274c81ef602",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer4': DisentangledTransformer(\n",
      "  (att_module): AttentionModule(\n",
      "    (gam_attention): GAM_Attention(\n",
      "      (channel_attention): Sequential(\n",
      "        (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      )\n",
      "      (spatial_attention): Sequential(\n",
      "        (0): Conv2d(2048, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(512, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "        (4): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (self_att_generator): SelfAttentionMap(\n",
      "      (W_k): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (W_q): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (softmax): Softmax(dim=2)\n",
      "    )\n",
      "    (global_att_generator): GlobalCrossAttentionMap(\n",
      "      (W_t): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (normalize): Softmax(dim=1)\n",
      "    )\n",
      "    (merge): Conv2d(2560, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (W_v): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (W_r): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (att_module2): AttentionModule(\n",
      "    (gam_attention): GAM_Attention(\n",
      "      (channel_attention): Sequential(\n",
      "        (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      )\n",
      "      (spatial_attention): Sequential(\n",
      "        (0): Conv2d(2048, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(512, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "        (4): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (self_att_generator): SelfAttentionMap(\n",
      "      (W_k): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (W_q): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (softmax): Softmax(dim=2)\n",
      "    )\n",
      "    (global_att_generator): GlobalCrossAttentionMap(\n",
      "      (W_t): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (normalize): Softmax(dim=1)\n",
      "    )\n",
      "    (merge): Conv2d(2560, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (W_v): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (W_r): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (global_styler): GlobalStyleTransformer2(\n",
      "    (global_transform): EqualLinear(\n",
      "      (linear): Linear(in_features=512, out_features=4096, bias=True)\n",
      "    )\n",
      "    (gate): EqualLinear(\n",
      "      (linear): Linear(in_features=512, out_features=4096, bias=True)\n",
      "    )\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (instance_norm): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      ")}\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "{'layer4': DisentangledTransformer(\n",
    "  (att_module): AttentionModule(\n",
    "    (gam_attention): GAM_Attention(\n",
    "      (channel_attention): Sequential(\n",
    "        (0): Linear(in_features=2048, out_features=512, bias=True)\n",
    "        (1): ReLU(inplace=True)\n",
    "        (2): Linear(in_features=512, out_features=2048, bias=True)\n",
    "      )\n",
    "      (spatial_attention): Sequential(\n",
    "        (0): Conv2d(2048, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
    "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (2): ReLU(inplace=True)\n",
    "        (3): Conv2d(512, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
    "        (4): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      )\n",
    "    )\n",
    "    (self_att_generator): SelfAttentionMap(\n",
    "      (W_k): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (W_q): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (softmax): Softmax(dim=2)\n",
    "    )\n",
    "    (global_att_generator): GlobalCrossAttentionMap(\n",
    "      (W_t): Linear(in_features=512, out_features=2048, bias=True)\n",
    "      (normalize): Softmax(dim=1)\n",
    "    )\n",
    "    (merge): Conv2d(2560, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "    (W_v): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "    (W_r): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
    "  )\n",
    "  (att_module2): AttentionModule(\n",
    "    (gam_attention): GAM_Attention(\n",
    "      (channel_attention): Sequential(\n",
    "        (0): Linear(in_features=2048, out_features=512, bias=True)\n",
    "        (1): ReLU(inplace=True)\n",
    "        (2): Linear(in_features=512, out_features=2048, bias=True)\n",
    "      )\n",
    "      (spatial_attention): Sequential(\n",
    "        (0): Conv2d(2048, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
    "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (2): ReLU(inplace=True)\n",
    "        (3): Conv2d(512, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
    "        (4): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      )\n",
    "    )\n",
    "    (self_att_generator): SelfAttentionMap(\n",
    "      (W_k): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (W_q): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (softmax): Softmax(dim=2)\n",
    "    )\n",
    "    (global_att_generator): GlobalCrossAttentionMap(\n",
    "      (W_t): Linear(in_features=512, out_features=2048, bias=True)\n",
    "      (normalize): Softmax(dim=1)\n",
    "    )\n",
    "    (merge): Conv2d(2560, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "    (W_v): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "    (W_r): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
    "  )\n",
    "  (global_styler): GlobalStyleTransformer2(\n",
    "    (global_transform): EqualLinear(\n",
    "      (linear): Linear(in_features=512, out_features=4096, bias=True)\n",
    "    )\n",
    "    (gate): EqualLinear(\n",
    "      (linear): Linear(in_features=512, out_features=4096, bias=True)\n",
    "    )\n",
    "    (sigmoid): Sigmoid()\n",
    "  )\n",
    "  (instance_norm): InstanceNorm2d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
    ")}\n",
    "```"
   ],
   "id": "db6f9bbfa3625de4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "be6d8eb142c89e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
