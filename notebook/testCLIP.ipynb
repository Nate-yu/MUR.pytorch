{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:24:41.116002Z",
     "start_time": "2024-06-19T06:24:38.362194Z"
    }
   },
   "source": [
    "from collections import OrderedDict\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from typing import List, Tuple, Union\n",
    "import hashlib\n",
    "import urllib\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from models.image_encoders.modified_resnet import ModifiedResNet\n",
    "from models.image_encoders.vit import VisionTransformer,Transformer,LayerNorm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:25:47.285399Z",
     "start_time": "2024-06-19T06:25:47.270267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 包含了不同CLIP模型的URL，用于下载预训练的模型\n",
    "_MODELS = {\n",
    "    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n",
    "    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n",
    "    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n",
    "    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n",
    "    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n",
    "}\n",
    "\n",
    "'''返回可用的模型名称列表'''\n",
    "def available_models() -> List[str]:\n",
    "    return list(_MODELS.keys())"
   ],
   "id": "1e8e2edbc5f7164f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:25:48.341926Z",
     "start_time": "2024-06-19T06:25:48.326665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _download(url: str, root: str):\n",
    "    os.makedirs(root, exist_ok=True)\n",
    "    filename = os.path.basename(url)\n",
    "\n",
    "    expected_sha256 = url.split(\"/\")[-2]\n",
    "    download_target = os.path.join(root, filename)\n",
    "\n",
    "    if os.path.exists(download_target) and not os.path.isfile(download_target):\n",
    "        raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n",
    "\n",
    "    if os.path.isfile(download_target):\n",
    "        if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest() == expected_sha256:\n",
    "            return download_target\n",
    "        else:\n",
    "            warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n",
    "\n",
    "    with urllib.request.urlopen(url) as source, open(download_target, \"wb\") as output:\n",
    "        with tqdm(total=int(source.info().get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:\n",
    "            while True:\n",
    "                buffer = source.read(8192)\n",
    "                if not buffer:\n",
    "                    break\n",
    "\n",
    "                output.write(buffer)\n",
    "                loop.update(len(buffer))\n",
    "\n",
    "    if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest() != expected_sha256:\n",
    "        raise RuntimeError(f\"Model has been downloaded but the SHA256 checksum does not not match\")\n",
    "\n",
    "    return download_target"
   ],
   "id": "9ea92a3326a37384",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:27:18.396234Z",
     "start_time": "2024-06-19T06:27:18.368346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 # vision\n",
    "                 image_resolution: Union[int, Tuple[int, int]],\n",
    "                 vision_layers: Union[Tuple[int, int, int, int], int],\n",
    "                 vision_width: int,\n",
    "                 vision_patch_size: int,\n",
    "                 stride_size: int,\n",
    "                 # text\n",
    "                 context_length: int,\n",
    "                 vocab_size: int,\n",
    "                 transformer_width: int,\n",
    "                 transformer_heads: int,\n",
    "                 transformer_layers: int\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "        if isinstance(vision_layers, (tuple, list)):\n",
    "            vision_heads = vision_width * 32 // 64\n",
    "            self.visual = ModifiedResNet(\n",
    "                layers=vision_layers,\n",
    "                output_dim=embed_dim,\n",
    "                heads=vision_heads,\n",
    "                input_resolution=image_resolution,\n",
    "                width=vision_width\n",
    "            )\n",
    "        else:\n",
    "            vision_heads = vision_width // 64\n",
    "            print(\"Vision heads: \", vision_heads)\n",
    "            self.visual = VisionTransformer(\n",
    "                input_resolution=image_resolution,\n",
    "                patch_size=vision_patch_size,\n",
    "                stride_size=stride_size,\n",
    "                width=vision_width,\n",
    "                layers=vision_layers,\n",
    "                heads=vision_heads,\n",
    "                output_dim=embed_dim\n",
    "            )\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            width=transformer_width,\n",
    "            layers=transformer_layers,\n",
    "            heads=transformer_heads,\n",
    "            attn_mask=self.build_attention_mask()\n",
    "        )\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
    "        self.ln_final = LayerNorm(transformer_width)\n",
    "\n",
    "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
    "        # self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
    "\n",
    "        if isinstance(self.visual, ModifiedResNet):\n",
    "            if self.visual.attnpool is not None:\n",
    "                std = self.visual.attnpool.c_proj.in_features ** -0.5\n",
    "                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n",
    "\n",
    "            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:\n",
    "                for name, param in resnet_block.named_parameters():\n",
    "                    if name.endswith(\"bn3.weight\"):\n",
    "                        nn.init.zeros_(param)\n",
    "\n",
    "        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
    "        attn_std = self.transformer.width ** -0.5\n",
    "        fc_std = (2 * self.transformer.width) ** -0.5\n",
    "        for block in self.transformer.resblocks:\n",
    "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
    "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
    "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
    "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
    "\n",
    "        if self.text_projection is not None:\n",
    "            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
    "\n",
    "    def build_attention_mask(self):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(self.context_length, self.context_length)\n",
    "        mask.fill_(float(\"-inf\"))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        return mask\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.conv1.weight.dtype\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        x = x + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        x = x @ self.text_projection\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(text)\n",
    "\n",
    "        return image_features, text_features\n",
    "    \n",
    "    def load_param(self, state_dict):\n",
    "        # 将pretrained_dict里不属于model_dict的键剔除掉\n",
    "        param_dict =  {k: v for k, v in state_dict.items() if k in self.state_dict()}\n",
    "\n",
    "        if 'model' in param_dict:\n",
    "            param_dict = param_dict['model']\n",
    "        if 'state_dict' in param_dict:\n",
    "            param_dict = param_dict['state_dict']\n",
    "        for k, v in param_dict.items():\n",
    "            if k == 'visual.positional_embedding' and v.shape != self.visual.positional_embedding.shape:\n",
    "                v = resize_pos_embed(v, self.visual.positional_embedding, self.visual.num_y, self.visual.num_x)\n",
    "            elif k == 'positional_embedding' and v.shape != self.positional_embedding.shape:\n",
    "                v = resize_text_pos_embed(v, self.context_length)\n",
    "            try:\n",
    "                self.state_dict()[k].copy_(v)\n",
    "            except:\n",
    "                print(f'===========================ERROR occur in copy {k}, {v.shape}=========================')\n",
    "                print('shape do not match in k :{}: param_dict{} vs self.state_dict(){}'.format(k, v.shape, self.state_dict()[k].shape))\n",
    "def resize_pos_embed(posemb, posemb_new, hight, width):\n",
    "    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
    "    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
    "    posemb = posemb.unsqueeze(0)\n",
    "    posemb_new = posemb_new.unsqueeze(0)\n",
    "\n",
    "    posemb_token, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
    "\n",
    "    gs_old = int(math.sqrt(len(posemb_grid)))\n",
    "    print('Resized position embedding from size:{} to size: {} with height:{} width: {}'.format(posemb.shape, posemb_new.shape, hight, width))\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=(hight, width), mode='bilinear')\n",
    "    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, hight * width, -1)\n",
    "    posemb = torch.cat([posemb_token, posemb_grid], dim=1)\n",
    "    return posemb.squeeze(0)\n",
    "    "
   ],
   "id": "c766fdce0f5d4066",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:27:19.139986Z",
     "start_time": "2024-06-19T06:27:19.125950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import OrderedDict\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from typing import List, Tuple, Union\n",
    "import hashlib\n",
    "import urllib\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import torch\n",
    "from torch import nn\n",
    "from models.image_encoders.modified_resnet import ModifiedResNet\n",
    "from models.image_encoders.vit import VisionTransformer,Transformer,LayerNorm\n",
    "logger = logging.getLogger(\"IRRA.model\")"
   ],
   "id": "e23eb1f9f2cce0fa",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:27:19.864651Z",
     "start_time": "2024-06-19T06:27:19.851616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_CLIP_from_openai_pretrained(name: str, image_size: Union[int, Tuple[int, int]], stride_size: int, jit: bool = False, download_root: str = None):\n",
    "    \"\"\"Load a CLIP model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        由' clip.available_models() '列出的模型名，或者包含state_dict的模型checkpoint的路径\n",
    "    \n",
    "    image_size: Union[int, Tuple[int, int]]\n",
    "        输入图像大小，224x224\n",
    "\n",
    "    jit : bool\n",
    "        是加载优化的JIT模型还是更容易被破解的非JIT模型(默认)。\n",
    "\n",
    "    download_root: str\n",
    "        模型文件下载路径；默认情况下，它使用“~/.cache/clip”\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : torch.nn.Module\n",
    "        The CLIP model\n",
    "    \"\"\"\n",
    "    if name in _MODELS:\n",
    "        model_path = _download(_MODELS[name], download_root or os.path.expanduser(\"~/.cache/clip\"))\n",
    "    elif os.path.isfile(name):\n",
    "        model_path = name\n",
    "    else:\n",
    "        raise RuntimeError(f\"Model {name} not found; available models = {available_models()}\")\n",
    "\n",
    "    try:\n",
    "        # 加载JIT存档\n",
    "        model = torch.jit.load(model_path, map_location=\"cpu\")\n",
    "        state_dict = None\n",
    "    except RuntimeError:\n",
    "        # 加载已保存的状态字典\n",
    "        if jit:\n",
    "            warnings.warn(f\"File {model_path} is not a JIT archive. Loading as a state dict instead\")\n",
    "            jit = False\n",
    "        state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "\n",
    "    state_dict = state_dict or model.state_dict()\n",
    "\n",
    "    vit = \"visual.proj\" in state_dict\n",
    "\n",
    "    if vit:\n",
    "        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n",
    "        print(\"Vision width:\", vision_width)\n",
    "        vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n",
    "        print(\"Vision layers:\", vision_layers)\n",
    "        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n",
    "        print(\"Vision patch size:\", vision_patch_size)\n",
    "        grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "        image_resolution = vision_patch_size * grid_size\n",
    "        print(\"Image resolution:\", image_resolution)\n",
    "    else:\n",
    "        counts: list = [len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]\n",
    "        vision_layers = tuple(counts)\n",
    "        \n",
    "        vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n",
    "        \n",
    "        output_width = round((state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "        \n",
    "        vision_patch_size = None\n",
    "        assert output_width ** 2 + 1 == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n",
    "        image_resolution = output_width * 32\n",
    "        \n",
    "\n",
    "    embed_dim = state_dict[\"text_projection\"].shape[1]\n",
    "    print(\"embed_dim: \", embed_dim)\n",
    "    context_length = state_dict[\"positional_embedding\"].shape[0]\n",
    "    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n",
    "    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n",
    "    transformer_heads = transformer_width // 64\n",
    "    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"transformer.resblocks\")))\n",
    "\n",
    "    model_cfg = {\n",
    "        'embed_dim': embed_dim,\n",
    "        'image_resolution': image_resolution,\n",
    "        'vision_layers': vision_layers, \n",
    "        'vision_width': vision_width, \n",
    "        'vision_patch_size': vision_patch_size,\n",
    "        'context_length': context_length, \n",
    "        'vocab_size': vocab_size, \n",
    "        'transformer_width': transformer_width, \n",
    "        'transformer_heads': transformer_heads, \n",
    "        'transformer_layers': transformer_layers\n",
    "    }\n",
    "\n",
    "\n",
    "    # modify image resolution to adapt Re-ID task\n",
    "    model_cfg['image_resolution'] = image_size\n",
    "    model_cfg['stride_size'] = stride_size\n",
    "    logger.info(f\"Load pretrained {name} CLIP model with model config: {model_cfg}\")\n",
    "    model = CLIP(**model_cfg)\n",
    "\n",
    "    model.load_param(state_dict)\n",
    "    return model, model_cfg"
   ],
   "id": "a1336ee22c1deecc",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:27:23.834331Z",
     "start_time": "2024-06-19T06:27:21.260686Z"
    }
   },
   "cell_type": "code",
   "source": "model, model_cfg = build_CLIP_from_openai_pretrained(\"ViT-B/16\",image_size=(224,224),stride_size=16)",
   "id": "ed693f94b2bf077f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision width: 768\n",
      "Vision layers: 12\n",
      "Vision patch size: 16\n",
      "Image resolution: 224\n",
      "embed_dim:  512\n",
      "Vision heads:  12\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2711c4f2865a011f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
