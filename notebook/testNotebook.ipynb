{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "pwd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_path = './image/1.png'\n",
    "def _get_img_from_path(img_path, transform=None):\n",
    "    with open(img_path, 'rb') as f:\n",
    "        img = Image.open(f).convert('RGB')\n",
    "    if transform is not None:\n",
    "        img = transform(img)\n",
    "    return img"
   ],
   "id": "7a0f627d7e697520",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "img = _get_img_from_path(img_path)\n",
    "img.show()\n",
    "print(img)"
   ],
   "id": "3f0a561b3a3263f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T07:22:07.520195Z",
     "start_time": "2024-06-03T07:22:03.308771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 测试resnet\n",
    "from models.image_encoders.resnet import ResNet18Layer4Upper, ResNet18Layer4Lower\n",
    "pretrained = True\n",
    "lower_encoder = ResNet18Layer4Lower(pretrained)\n",
    "# print(type(lower_encoder))\n",
    "# print(lower_encoder.layer_shapes())\n",
    "lower_feature_shape = lower_encoder.layer_shapes()['layer4'] # 512\n",
    "feature_size = 512\n",
    "norm_scale = 4\n",
    "model = ResNet18Layer4Upper(lower_feature_shape, feature_size, pretrained=pretrained,norm_scale=norm_scale)\n",
    "# print(model)\n",
    "print(model.lower_feature_shape, model.feature_size) # 512 512\n",
    "\n",
    "# 结论：ResNet18Layer4Lower主要负责特征的初步提取，而ResNet18Layer4Upper则负责将这些特征转换为更高级的、用于后续任务的表示。\n",
    "#      这两个类设计为连续使用，其中ResNet18Layer4Lower的输出直接作为ResNet18Layer4Upper的输入。这种设计使得整个模型的两部分可以灵活地根据需要进行调整或替换。"
   ],
   "id": "c7fd5252132ddd",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown value 'True' for ResNet18_Weights.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_encoders\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mresnet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ResNet18Layer4Upper, ResNet18Layer4Lower\n\u001B[0;32m      3\u001B[0m pretrained \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTrue\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 4\u001B[0m lower_encoder \u001B[38;5;241m=\u001B[39m \u001B[43mResNet18Layer4Lower\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# print(type(lower_encoder))\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# print(lower_encoder.layer_shapes())\u001B[39;00m\n\u001B[0;32m      7\u001B[0m lower_feature_shape \u001B[38;5;241m=\u001B[39m lower_encoder\u001B[38;5;241m.\u001B[39mlayer_shapes()[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlayer4\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;66;03m# 512\u001B[39;00m\n",
      "File \u001B[1;32mD:\\CVCode\\my_project\\models\\image_encoders\\resnet.py:16\u001B[0m, in \u001B[0;36mResNet18Layer4Lower.__init__\u001B[1;34m(self, pretrained)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pretrained\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[1;32m---> 16\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model \u001B[38;5;241m=\u001B[39m \u001B[43mresnet18\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpretrained\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\IDE\\Miniconda\\envs\\cv\\lib\\site-packages\\torchvision\\models\\_utils.py:142\u001B[0m, in \u001B[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    135\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    136\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msequence_to_str(\u001B[38;5;28mtuple\u001B[39m(keyword_only_kwargs\u001B[38;5;241m.\u001B[39mkeys()),\u001B[38;5;250m \u001B[39mseparate_last\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mand \u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m as positional \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    137\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter(s) is deprecated since 0.13 and will be removed in 0.15. Please use keyword parameter(s) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    138\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    139\u001B[0m     )\n\u001B[0;32m    140\u001B[0m     kwargs\u001B[38;5;241m.\u001B[39mupdate(keyword_only_kwargs)\n\u001B[1;32m--> 142\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\IDE\\Miniconda\\envs\\cv\\lib\\site-packages\\torchvision\\models\\_utils.py:228\u001B[0m, in \u001B[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    225\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m kwargs[pretrained_param]\n\u001B[0;32m    226\u001B[0m     kwargs[weights_param] \u001B[38;5;241m=\u001B[39m default_weights_arg\n\u001B[1;32m--> 228\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbuilder\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\IDE\\Miniconda\\envs\\cv\\lib\\site-packages\\torchvision\\models\\resnet.py:668\u001B[0m, in \u001B[0;36mresnet18\u001B[1;34m(weights, progress, **kwargs)\u001B[0m\n\u001B[0;32m    648\u001B[0m \u001B[38;5;129m@handle_legacy_interface\u001B[39m(weights\u001B[38;5;241m=\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpretrained\u001B[39m\u001B[38;5;124m\"\u001B[39m, ResNet18_Weights\u001B[38;5;241m.\u001B[39mIMAGENET1K_V1))\n\u001B[0;32m    649\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mresnet18\u001B[39m(\u001B[38;5;241m*\u001B[39m, weights: Optional[ResNet18_Weights] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, progress: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResNet:\n\u001B[0;32m    650\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"ResNet-18 from `Deep Residual Learning for Image Recognition <https://arxiv.org/pdf/1512.03385.pdf>`__.\u001B[39;00m\n\u001B[0;32m    651\u001B[0m \n\u001B[0;32m    652\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    666\u001B[0m \u001B[38;5;124;03m        :members:\u001B[39;00m\n\u001B[0;32m    667\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 668\u001B[0m     weights \u001B[38;5;241m=\u001B[39m \u001B[43mResNet18_Weights\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverify\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweights\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    670\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _resnet(BasicBlock, [\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m], weights, progress, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\IDE\\Miniconda\\envs\\cv\\lib\\site-packages\\torchvision\\models\\_api.py:55\u001B[0m, in \u001B[0;36mWeightsEnum.verify\u001B[1;34m(cls, obj)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(obj) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m---> 55\u001B[0m         obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_str\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplace\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, \u001B[38;5;28mcls\u001B[39m):\n\u001B[0;32m     57\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m     58\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid Weight class provided; expected \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m but received \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mobj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     59\u001B[0m         )\n",
      "File \u001B[1;32mD:\\IDE\\Miniconda\\envs\\cv\\lib\\site-packages\\torchvision\\_utils.py:16\u001B[0m, in \u001B[0;36mStrEnumMeta.from_str\u001B[1;34m(self, member)\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m[member]\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;66;03m# TODO: use `add_suggestion` from torchvision.prototype.utils._internal to improve the error message as\u001B[39;00m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;66;03m#  soon as it is migrated.\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown value \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmember\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: Unknown value 'True' for ResNet18_Weights."
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "def create_read_func(vocab_path):\n",
    "    # 用于从给定的路径 vocab_path 读取并反序列化数据\n",
    "    def read_func():\n",
    "        with open(vocab_path, 'rb') as f:\n",
    "            # 使用了 Python 的 pickle 模块来加载存储的对象\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "\n",
    "    return read_func\n",
    "read_func = create_read_func('D:/Datasets/fashionIQ/fashion_iq_vocab.pkl')\n",
    "data = read_func()\n",
    "print(type(data))"
   ],
   "id": "fb5d09e49b32aead",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 测试RoBerta\n",
    "from models.text_encoders.roberta import RobertaEncoder,BertFc\n",
    "model1, model2 = RobertaEncoder(512), BertFc(512)\n",
    "print(model1, model2)\n"
   ],
   "id": "81dc2cbd57be7406",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T12:57:19.520826Z",
     "start_time": "2024-05-12T12:57:19.503531Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "978e7badc8e3f00c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:12:01.652946Z",
     "start_time": "2024-05-15T13:12:00.893743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class GlobalCrossAttentionMap(nn.Module):\n",
    "    def __init__(self, feature_size, text_feature_size, num_heads, normalizer=None, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # 注意力头的数量\n",
    "        self.n_heads = num_heads\n",
    "        # 每个注意力头处理的特征维度，计算为 feature_size 除以 num_heads\n",
    "        self.c_per_head = feature_size // num_heads\n",
    "        assert feature_size == self.n_heads * self.c_per_head\n",
    "\n",
    "        # 用于将文本特征映射到与图像特征相同的维度\n",
    "        self.W_t = nn.Linear(text_feature_size, feature_size)\n",
    "        self.normalize = normalizer if normalizer else nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # 将图像的四个维度剥离保存，batch_size,chanel,height,weight\n",
    "        b, c, h, w = x.size()\n",
    "        # x 的维度被重塑以适应注意力头的处理，首先按照注意力头数量和每头的特征维度重塑\n",
    "        x_reshape = x.view(b * self.n_heads, self.c_per_head, h, w)\n",
    "        # 然后将高度和宽度合并\n",
    "        x_reshape = x_reshape.view(b * self.n_heads, self.c_per_head, h * w)\n",
    "\n",
    "        # 文本特征 t 通过 self.W_t 映射\n",
    "        t_mapped = self.W_t(t)\n",
    "        # 并调整维度以匹配处理后的图像特征 x_reshape\n",
    "        t_mapped = t_mapped.view(b * self.n_heads, self.c_per_head, 1)\n",
    "\n",
    "        # 使用批量矩阵乘法 (torch.bmm) 计算注意力映射，然后通过根号缩放因子进行缩放\n",
    "        att_map = torch.bmm(x_reshape.transpose(1, 2), t_mapped).squeeze(-1) / (self.c_per_head ** 0.5)\n",
    "        # 应用归一化函数（默认为 Softmax）来归一化注意力映射\n",
    "        att_map = self.normalize(att_map)  # (b * n_heads, h * w)\n",
    "        # 注意力映射的维度被调整回原始的批次大小和注意力头数量\n",
    "        att_map = att_map.view(b * self.n_heads, 1, h * w)\n",
    "        att_map = att_map.view(b, self.n_heads, h * w)\n",
    "\n",
    "        return att_map"
   ],
   "id": "2d452e2a3304dea7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:21:31.846261Z",
     "start_time": "2024-05-15T13:21:31.833287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SelfAttentionMap(nn.Module):\n",
    "    def __init__(self, feature_size, num_heads, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = num_heads\n",
    "        self.c_per_head = feature_size // num_heads\n",
    "        assert feature_size == self.n_heads * self.c_per_head\n",
    "\n",
    "        self.W_k = nn.Conv2d(feature_size, feature_size, kernel_size=1, bias=False)\n",
    "        self.W_q = nn.Conv2d(feature_size, feature_size, kernel_size=1, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        b, c, h, w = x.size()\n",
    "\n",
    "        keys, queries = self.W_k(x), self.W_q(x)\n",
    "        keys = keys.view(b * self.n_heads, self.c_per_head, h, w).view(b * self.n_heads, self.c_per_head, h * w)\n",
    "        queries = queries.view(b * self.n_heads, self.c_per_head, h, w).view(b * self.n_heads, self.c_per_head, h * w)\n",
    "\n",
    "        att_map = torch.bmm(queries.transpose(1, 2), keys) / (self.c_per_head ** 0.5)\n",
    "        att_map = self.softmax(att_map)  # (b * num_heads, h * w, h * w), torch.sum(att_map[batch_idx][?]) == 1\n",
    "        att_map = att_map.view(b, self.n_heads, h * w, h * w)\n",
    "\n",
    "        return att_map"
   ],
   "id": "358b3c2d303ef86b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:21:33.863319Z",
     "start_time": "2024-05-15T13:21:33.842227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reshape_text_features_to_concat(text_features, image_features_shapes):\n",
    "    return text_features.view((*text_features.size(), 1, 1)).repeat(1, 1, *image_features_shapes[2:])\n",
    "from models.attention_modules.external_attention import ExternalAttention\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, feature_size, text_feature_size, num_heads, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = num_heads\n",
    "        self.c_per_head = feature_size // num_heads\n",
    "        assert feature_size == self.n_heads * self.c_per_head\n",
    "        \n",
    "        # 两个组件的初始化\n",
    "        self.self_att_generator = SelfAttentionMap(feature_size, num_heads, *args, **kwargs)\n",
    "        self.external_att = ExternalAttention(d_model= feature_size, S= 64)\n",
    "        self.global_att_generator = GlobalCrossAttentionMap(feature_size, text_feature_size, num_heads, *args, **kwargs)\n",
    "\n",
    "        # 合并图像和调整形状后的文本特征\n",
    "        self.merge = nn.Conv2d(feature_size + text_feature_size, feature_size, kernel_size=1, bias=False)\n",
    "        # 用于生成值（values）\n",
    "        self.W_v = nn.Conv2d(feature_size, feature_size, kernel_size=1, bias=False)\n",
    "        # 用于最终输出的调整\n",
    "        self.W_r = nn.Conv2d(feature_size, feature_size, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, t, return_map=False, *args, **kwargs):\n",
    "        b, c, h, w = x.size()\n",
    "        \n",
    "        # 将文本特征调整形状以匹配图像特征的维度\n",
    "        t_reshaped = reshape_text_features_to_concat(t, x.size())\n",
    "        # 将图像特征和调整形状后的文本特征合并后，通过卷积层merge处理\n",
    "        vl_features = self.merge(torch.cat([x, t_reshaped], dim=1))  # (b, c, h, w)\n",
    "\n",
    "        # 通过卷积层W_v处理合并后的特征，用于生成注意力机制中的值（values）\n",
    "        values = self.W_v(vl_features)\n",
    "        values = values.view(b * self.n_heads, self.c_per_head, h, w).view(b * self.n_heads, self.c_per_head, h * w)\n",
    "        \n",
    "        # 通过外部注意力模块处理图像特征\n",
    "        external_att_out = self.external_att(x.view(b * h * w, c))  # (b * h * w, c)\n",
    "        external_att_out = external_att_out.view(b, h, w, c).permute(0, 3, 1, 2)  # (b, c, h, w)\n",
    "\n",
    "        # 通过自注意力生成器计算自注意力映射图\n",
    "        self_att_map = self.self_att_generator(x)  # (b, num_heads, h * w, h * w)\n",
    "        # 通过全局交叉注意力生成器计算全局交叉注意力映射图\n",
    "        global_cross_att_map = self.global_att_generator(x, t)\n",
    "        global_cross_att_map = global_cross_att_map.view(b, self.n_heads, 1, h * w)  # (b, num_heads, 1, h * w)\n",
    "        #  将自注意力图和全局交叉注意力图相加，得到最终的注意力图\n",
    "        att_map = self_att_map + global_cross_att_map  # (b, num_heads, h * w, h * w)\n",
    "        att_map_reshaped = att_map.view(b * self.n_heads, h * w, h * w)  # (b * num_heads, h * w, h * w)\n",
    "\n",
    "        # 使用注意力图重新加权values，通过矩阵乘法和重塑操作处理，最后通过卷积层W_r调整输出\n",
    "        att_out = torch.bmm(values, att_map_reshaped.transpose(1, 2))  # (b * num_heads, c_per_head, h * w)\n",
    "        att_out = att_out.view(b, self.n_heads * self.c_per_head, h * w)\n",
    "        att_out = att_out.view(b, self.n_heads * self.c_per_head, h, w)\n",
    "        att_out = self.W_r(att_out + external_att_out)  # 将外部注意力的输出与现有的注意力输出相加\n",
    "\n",
    "        return att_out, att_map if return_map else att_out"
   ],
   "id": "81f2d069e0b044a6",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:21:35.208448Z",
     "start_time": "2024-05-15T13:21:35.190382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "att = AttentionModule(feature_size=512, text_feature_size= 512, num_heads=2, normalizer=nn.LayerNorm)\n",
    "print(att)"
   ],
   "id": "55b0bd32cd864349",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionModule(\n",
      "  (self_att_generator): SelfAttentionMap(\n",
      "    (W_k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (W_q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (softmax): Softmax(dim=2)\n",
      "  )\n",
      "  (external_att): ExternalAttention(\n",
      "    (mk): Linear(in_features=512, out_features=64, bias=False)\n",
      "    (mv): Linear(in_features=64, out_features=512, bias=False)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (global_att_generator): GlobalCrossAttentionMap(\n",
      "    (W_t): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (merge): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (W_v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (W_r): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T11:57:45.488546Z",
     "start_time": "2024-05-14T11:57:44.055520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "'''为特征加入高斯噪声'''\n",
    "class NormalAugmenter(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_size, alpha_scale=1, beta_scale=1, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.instance_norm = nn.InstanceNorm1d(feature_size) # 用于对特征进行归一化处理\n",
    "        self.alpha_scale = alpha_scale # 1\n",
    "        self.beta_scale = beta_scale # 1\n",
    "\n",
    "    def forward(self, features, *args, **kwargs):\n",
    "        std, mean = torch.std_mean(features, dim=1)\n",
    "        normal_alpha = torch.distributions.Normal(loc=1, scale=std)\n",
    "        normal_beta = torch.distributions.Normal(loc=mean, scale=std)\n",
    "        alpha = self.alpha_scale * normal_alpha.sample([features.shape[1]]).transpose(-1, -2)\n",
    "        beta = self.beta_scale * normal_beta.sample([features.shape[1]]).transpose(-1, -2)\n",
    "\n",
    "        features = self.instance_norm(features)\n",
    "        x = alpha * features + beta\n",
    "        return x\n",
    "\n",
    "    @classmethod\n",
    "    def code(cls) -> str:\n",
    "        return 'normal_gaussian'"
   ],
   "id": "dc35248e8a8ef9d9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T11:57:47.189696Z",
     "start_time": "2024-05-14T11:57:47.185685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = NormalAugmenter(512)\n",
    "print(model)"
   ],
   "id": "289cbc8211a5f056",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NormalAugmenter(\n",
      "  (instance_norm): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T12:19:54.889861Z",
     "start_time": "2024-05-14T12:19:53.319374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models.attention_modules.self_attention import AttentionModule\n",
    "class DisentangledTransformer(nn.Module):\n",
    "    def __init__(self, feature_size, text_feature_size, num_heads, global_styler=None, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.n_heads = num_heads\n",
    "        self.c_per_head = feature_size // num_heads\n",
    "        assert feature_size == self.n_heads * self.c_per_head\n",
    "\n",
    "        self.att_module = AttentionModule(feature_size, text_feature_size, num_heads, *args, **kwargs)\n",
    "        self.att_module2 = AttentionModule(feature_size, text_feature_size, num_heads, *args, **kwargs)\n",
    "        self.global_styler = global_styler\n",
    "\n",
    "        self.weights = nn.Parameter(torch.tensor([1., 1.]))\n",
    "        self.instance_norm = nn.InstanceNorm2d(feature_size)\n",
    "\n",
    "    def forward(self, x, t, *args, **kwargs):\n",
    "        normed_x = self.instance_norm(x)\n",
    "        att_out, att_map = self.att_module(normed_x, t, return_map=True)\n",
    "        out = normed_x + self.weights[0] * att_out\n",
    "\n",
    "        att_out2, att_map2 = self.att_module2(out, t, return_map=True)\n",
    "        out = out + self.weights[1] * att_out2\n",
    "\n",
    "        out = self.global_styler(out, t, x=x)\n",
    "\n",
    "        return out, att_map"
   ],
   "id": "b7a25e3d26e8c522",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T12:20:27.892178Z",
     "start_time": "2024-05-14T12:20:27.870120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = DisentangledTransformer(512,512,2)\n",
    "print(model)"
   ],
   "id": "e3f39f4268a8baba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DisentangledTransformer(\n",
      "  (att_module): AttentionModule(\n",
      "    (self_att_generator): SelfAttentionMap(\n",
      "      (W_k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (W_q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (softmax): Softmax(dim=2)\n",
      "    )\n",
      "    (global_att_generator): GlobalCrossAttentionMap(\n",
      "      (W_t): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (normalize): Softmax(dim=1)\n",
      "    )\n",
      "    (merge): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (W_v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (W_r): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (att_module2): AttentionModule(\n",
      "    (self_att_generator): SelfAttentionMap(\n",
      "      (W_k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (W_q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (softmax): Softmax(dim=2)\n",
      "    )\n",
      "    (global_att_generator): GlobalCrossAttentionMap(\n",
      "      (W_t): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (normalize): Softmax(dim=1)\n",
      "    )\n",
      "    (merge): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (W_v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (W_r): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (instance_norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T13:31:11.107833Z",
     "start_time": "2024-05-28T13:31:09.991325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# 使用tqdm显示一个简单的循环进度条\n",
    "for i in tqdm(range(10)):\n",
    "    time.sleep(0.1)  # 模拟耗时操作\n"
   ],
   "id": "7b5ea018d8f24e58",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.11it/s]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T07:49:31.450616Z",
     "start_time": "2024-05-29T07:49:30.298183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# 使用更多参数自定义进度条\n",
    "for i in tqdm(range(10), desc=\"Processing\", unit=\"step\"):\n",
    "    time.sleep(0.1)  # 模拟耗时操作\n"
   ],
   "id": "2c66b2e7fa7a456e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 10/10 [00:01<00:00,  9.09step/s]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T08:49:16.483011Z",
     "start_time": "2024-05-29T08:49:15.594339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# 1. 查看预训练权重的关键字\n",
    "pretrained_weights_path = './ckpt/resnet18.pth'\n",
    "state_dict = torch.load(pretrained_weights_path)\n",
    "# 获取预训练权重的关键字\n",
    "pretrained_keys = state_dict.keys()\n",
    "print(\"预训练权重关键字：\")\n",
    "for key in state_dict.keys():\n",
    "    print(key)"
   ],
   "id": "5be3a713d1c2e015",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预训练权重关键字：\n",
      "conv1.weight\n",
      "bn1.running_mean\n",
      "bn1.running_var\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "layer1.0.conv1.weight\n",
      "layer1.0.bn1.running_mean\n",
      "layer1.0.bn1.running_var\n",
      "layer1.0.bn1.weight\n",
      "layer1.0.bn1.bias\n",
      "layer1.0.conv2.weight\n",
      "layer1.0.bn2.running_mean\n",
      "layer1.0.bn2.running_var\n",
      "layer1.0.bn2.weight\n",
      "layer1.0.bn2.bias\n",
      "layer1.1.conv1.weight\n",
      "layer1.1.bn1.running_mean\n",
      "layer1.1.bn1.running_var\n",
      "layer1.1.bn1.weight\n",
      "layer1.1.bn1.bias\n",
      "layer1.1.conv2.weight\n",
      "layer1.1.bn2.running_mean\n",
      "layer1.1.bn2.running_var\n",
      "layer1.1.bn2.weight\n",
      "layer1.1.bn2.bias\n",
      "layer2.0.conv1.weight\n",
      "layer2.0.bn1.running_mean\n",
      "layer2.0.bn1.running_var\n",
      "layer2.0.bn1.weight\n",
      "layer2.0.bn1.bias\n",
      "layer2.0.conv2.weight\n",
      "layer2.0.bn2.running_mean\n",
      "layer2.0.bn2.running_var\n",
      "layer2.0.bn2.weight\n",
      "layer2.0.bn2.bias\n",
      "layer2.0.downsample.0.weight\n",
      "layer2.0.downsample.1.running_mean\n",
      "layer2.0.downsample.1.running_var\n",
      "layer2.0.downsample.1.weight\n",
      "layer2.0.downsample.1.bias\n",
      "layer2.1.conv1.weight\n",
      "layer2.1.bn1.running_mean\n",
      "layer2.1.bn1.running_var\n",
      "layer2.1.bn1.weight\n",
      "layer2.1.bn1.bias\n",
      "layer2.1.conv2.weight\n",
      "layer2.1.bn2.running_mean\n",
      "layer2.1.bn2.running_var\n",
      "layer2.1.bn2.weight\n",
      "layer2.1.bn2.bias\n",
      "layer3.0.conv1.weight\n",
      "layer3.0.bn1.running_mean\n",
      "layer3.0.bn1.running_var\n",
      "layer3.0.bn1.weight\n",
      "layer3.0.bn1.bias\n",
      "layer3.0.conv2.weight\n",
      "layer3.0.bn2.running_mean\n",
      "layer3.0.bn2.running_var\n",
      "layer3.0.bn2.weight\n",
      "layer3.0.bn2.bias\n",
      "layer3.0.downsample.0.weight\n",
      "layer3.0.downsample.1.running_mean\n",
      "layer3.0.downsample.1.running_var\n",
      "layer3.0.downsample.1.weight\n",
      "layer3.0.downsample.1.bias\n",
      "layer3.1.conv1.weight\n",
      "layer3.1.bn1.running_mean\n",
      "layer3.1.bn1.running_var\n",
      "layer3.1.bn1.weight\n",
      "layer3.1.bn1.bias\n",
      "layer3.1.conv2.weight\n",
      "layer3.1.bn2.running_mean\n",
      "layer3.1.bn2.running_var\n",
      "layer3.1.bn2.weight\n",
      "layer3.1.bn2.bias\n",
      "layer4.0.conv1.weight\n",
      "layer4.0.bn1.running_mean\n",
      "layer4.0.bn1.running_var\n",
      "layer4.0.bn1.weight\n",
      "layer4.0.bn1.bias\n",
      "layer4.0.conv2.weight\n",
      "layer4.0.bn2.running_mean\n",
      "layer4.0.bn2.running_var\n",
      "layer4.0.bn2.weight\n",
      "layer4.0.bn2.bias\n",
      "layer4.0.downsample.0.weight\n",
      "layer4.0.downsample.1.running_mean\n",
      "layer4.0.downsample.1.running_var\n",
      "layer4.0.downsample.1.weight\n",
      "layer4.0.downsample.1.bias\n",
      "layer4.1.conv1.weight\n",
      "layer4.1.bn1.running_mean\n",
      "layer4.1.bn1.running_var\n",
      "layer4.1.bn1.weight\n",
      "layer4.1.bn1.bias\n",
      "layer4.1.conv2.weight\n",
      "layer4.1.bn2.running_mean\n",
      "layer4.1.bn2.running_var\n",
      "layer4.1.bn2.weight\n",
      "layer4.1.bn2.bias\n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T08:49:19.742652Z",
     "start_time": "2024-05-29T08:49:19.210402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. 查看自身网络模型的关键字\n",
    "from torchvision.models import resnet18\n",
    "net = resnet18()\n",
    "model_keys = net.state_dict().keys()\n",
    "\n",
    "print(\"模型权重的关键字：\")\n",
    "for key in model_keys:\n",
    "    print(key)"
   ],
   "id": "98505603bfef390b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型权重的关键字：\n",
      "conv1.weight\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "bn1.running_mean\n",
      "bn1.running_var\n",
      "bn1.num_batches_tracked\n",
      "layer1.0.conv1.weight\n",
      "layer1.0.bn1.weight\n",
      "layer1.0.bn1.bias\n",
      "layer1.0.bn1.running_mean\n",
      "layer1.0.bn1.running_var\n",
      "layer1.0.bn1.num_batches_tracked\n",
      "layer1.0.conv2.weight\n",
      "layer1.0.bn2.weight\n",
      "layer1.0.bn2.bias\n",
      "layer1.0.bn2.running_mean\n",
      "layer1.0.bn2.running_var\n",
      "layer1.0.bn2.num_batches_tracked\n",
      "layer1.1.conv1.weight\n",
      "layer1.1.bn1.weight\n",
      "layer1.1.bn1.bias\n",
      "layer1.1.bn1.running_mean\n",
      "layer1.1.bn1.running_var\n",
      "layer1.1.bn1.num_batches_tracked\n",
      "layer1.1.conv2.weight\n",
      "layer1.1.bn2.weight\n",
      "layer1.1.bn2.bias\n",
      "layer1.1.bn2.running_mean\n",
      "layer1.1.bn2.running_var\n",
      "layer1.1.bn2.num_batches_tracked\n",
      "layer2.0.conv1.weight\n",
      "layer2.0.bn1.weight\n",
      "layer2.0.bn1.bias\n",
      "layer2.0.bn1.running_mean\n",
      "layer2.0.bn1.running_var\n",
      "layer2.0.bn1.num_batches_tracked\n",
      "layer2.0.conv2.weight\n",
      "layer2.0.bn2.weight\n",
      "layer2.0.bn2.bias\n",
      "layer2.0.bn2.running_mean\n",
      "layer2.0.bn2.running_var\n",
      "layer2.0.bn2.num_batches_tracked\n",
      "layer2.0.downsample.0.weight\n",
      "layer2.0.downsample.1.weight\n",
      "layer2.0.downsample.1.bias\n",
      "layer2.0.downsample.1.running_mean\n",
      "layer2.0.downsample.1.running_var\n",
      "layer2.0.downsample.1.num_batches_tracked\n",
      "layer2.1.conv1.weight\n",
      "layer2.1.bn1.weight\n",
      "layer2.1.bn1.bias\n",
      "layer2.1.bn1.running_mean\n",
      "layer2.1.bn1.running_var\n",
      "layer2.1.bn1.num_batches_tracked\n",
      "layer2.1.conv2.weight\n",
      "layer2.1.bn2.weight\n",
      "layer2.1.bn2.bias\n",
      "layer2.1.bn2.running_mean\n",
      "layer2.1.bn2.running_var\n",
      "layer2.1.bn2.num_batches_tracked\n",
      "layer3.0.conv1.weight\n",
      "layer3.0.bn1.weight\n",
      "layer3.0.bn1.bias\n",
      "layer3.0.bn1.running_mean\n",
      "layer3.0.bn1.running_var\n",
      "layer3.0.bn1.num_batches_tracked\n",
      "layer3.0.conv2.weight\n",
      "layer3.0.bn2.weight\n",
      "layer3.0.bn2.bias\n",
      "layer3.0.bn2.running_mean\n",
      "layer3.0.bn2.running_var\n",
      "layer3.0.bn2.num_batches_tracked\n",
      "layer3.0.downsample.0.weight\n",
      "layer3.0.downsample.1.weight\n",
      "layer3.0.downsample.1.bias\n",
      "layer3.0.downsample.1.running_mean\n",
      "layer3.0.downsample.1.running_var\n",
      "layer3.0.downsample.1.num_batches_tracked\n",
      "layer3.1.conv1.weight\n",
      "layer3.1.bn1.weight\n",
      "layer3.1.bn1.bias\n",
      "layer3.1.bn1.running_mean\n",
      "layer3.1.bn1.running_var\n",
      "layer3.1.bn1.num_batches_tracked\n",
      "layer3.1.conv2.weight\n",
      "layer3.1.bn2.weight\n",
      "layer3.1.bn2.bias\n",
      "layer3.1.bn2.running_mean\n",
      "layer3.1.bn2.running_var\n",
      "layer3.1.bn2.num_batches_tracked\n",
      "layer4.0.conv1.weight\n",
      "layer4.0.bn1.weight\n",
      "layer4.0.bn1.bias\n",
      "layer4.0.bn1.running_mean\n",
      "layer4.0.bn1.running_var\n",
      "layer4.0.bn1.num_batches_tracked\n",
      "layer4.0.conv2.weight\n",
      "layer4.0.bn2.weight\n",
      "layer4.0.bn2.bias\n",
      "layer4.0.bn2.running_mean\n",
      "layer4.0.bn2.running_var\n",
      "layer4.0.bn2.num_batches_tracked\n",
      "layer4.0.downsample.0.weight\n",
      "layer4.0.downsample.1.weight\n",
      "layer4.0.downsample.1.bias\n",
      "layer4.0.downsample.1.running_mean\n",
      "layer4.0.downsample.1.running_var\n",
      "layer4.0.downsample.1.num_batches_tracked\n",
      "layer4.1.conv1.weight\n",
      "layer4.1.bn1.weight\n",
      "layer4.1.bn1.bias\n",
      "layer4.1.bn1.running_mean\n",
      "layer4.1.bn1.running_var\n",
      "layer4.1.bn1.num_batches_tracked\n",
      "layer4.1.conv2.weight\n",
      "layer4.1.bn2.weight\n",
      "layer4.1.bn2.bias\n",
      "layer4.1.bn2.running_mean\n",
      "layer4.1.bn2.running_var\n",
      "layer4.1.bn2.num_batches_tracked\n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T08:49:21.357643Z",
     "start_time": "2024-05-29T08:49:21.341684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. 找出模型中缺失的权重（针对预训练关键字比模型关键字少）\n",
    "missing_keys = model_keys - pretrained_keys\n",
    "print(\"模型中缺失的权重关键字：\")\n",
    "for key in missing_keys:\n",
    "    print(key)"
   ],
   "id": "3da485ec8108ae46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型中缺失的权重关键字：\n",
      "layer2.0.downsample.1.num_batches_tracked\n",
      "layer3.0.bn2.num_batches_tracked\n",
      "layer4.1.bn1.num_batches_tracked\n",
      "layer1.0.bn1.num_batches_tracked\n",
      "layer4.1.bn2.num_batches_tracked\n",
      "layer1.1.bn2.num_batches_tracked\n",
      "layer3.1.bn1.num_batches_tracked\n",
      "layer3.1.bn2.num_batches_tracked\n",
      "layer2.0.bn2.num_batches_tracked\n",
      "layer2.0.bn1.num_batches_tracked\n",
      "layer2.1.bn1.num_batches_tracked\n",
      "layer3.0.bn1.num_batches_tracked\n",
      "bn1.num_batches_tracked\n",
      "layer2.1.bn2.num_batches_tracked\n",
      "layer3.0.downsample.1.num_batches_tracked\n",
      "layer1.1.bn1.num_batches_tracked\n",
      "layer4.0.downsample.1.num_batches_tracked\n",
      "layer1.0.bn2.num_batches_tracked\n",
      "layer4.0.bn2.num_batches_tracked\n",
      "layer4.0.bn1.num_batches_tracked\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T08:49:24.376740Z",
     "start_time": "2024-05-29T08:49:24.360953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. 找出模型中多余的权重（针对预训练关键字比模型关键字多）\n",
    "unexpected_keys = pretrained_keys - model_keys\n",
    "print(\"模型中多余的权重关键字：\")\n",
    "for key in unexpected_keys:\n",
    "    print(key)"
   ],
   "id": "479a87bee17eb63b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型中多余的权重关键字：\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T08:49:25.301828Z",
     "start_time": "2024-05-29T08:49:25.280100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 关键字不匹配\n",
    "net.load_state_dict(state_dict)"
   ],
   "id": "a1637c9090935af0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T08:49:30.211923Z",
     "start_time": "2024-05-29T08:49:30.090172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. 解决关键字不匹配问题\n",
    "model_weights_path = './ckpt/resnet18.pth'\n",
    "# 加载预训练权重\n",
    "ckpt = torch.load(model_weights_path)\n",
    "net = resnet18()\n",
    "# 得到模型参数\n",
    "model_dict = net.state_dict()\n",
    "# 判断预训练模型中网络的模块是否修改后的网络中也存在，并且shape相同，如果相同则取出\n",
    "pretrained_dict = {k:v for k, v in ckpt.items() if k in model_dict and (v.shape == model_dict[k].shape)}\n",
    "# 更新之后的model_dict\n",
    "model_dict.update(pretrained_dict)\n",
    "# 加载真正需要的state_dict\n",
    "net.load_state_dict(model_dict, strict=True)"
   ],
   "id": "727433c43cb7b6d5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T08:52:48.931806Z",
     "start_time": "2024-05-29T08:52:47.541393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "pretrained_weights_path = './ckpt/resnet18.pth'\n",
    "state_dict = torch.load(pretrained_weights_path)\n",
    "from torchvision.models import resnet18,resnet50\n",
    "net = resnet50()\n",
    "model_keys = net.state_dict().keys()\n",
    "net.load_state_dict(state_dict)"
   ],
   "id": "e410dbfed232ee2e",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"layer1.0.conv3.weight\", \"layer1.0.bn3.weight\", \"layer1.0.bn3.bias\", \"layer1.0.bn3.running_mean\", \"layer1.0.bn3.running_var\", \"layer1.0.downsample.0.weight\", \"layer1.0.downsample.1.weight\", \"layer1.0.downsample.1.bias\", \"layer1.0.downsample.1.running_mean\", \"layer1.0.downsample.1.running_var\", \"layer1.1.conv3.weight\", \"layer1.1.bn3.weight\", \"layer1.1.bn3.bias\", \"layer1.1.bn3.running_mean\", \"layer1.1.bn3.running_var\", \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.bn2.bias\", \"layer1.2.bn2.running_mean\", \"layer1.2.bn2.running_var\", \"layer1.2.conv3.weight\", \"layer1.2.bn3.weight\", \"layer1.2.bn3.bias\", \"layer1.2.bn3.running_mean\", \"layer1.2.bn3.running_var\", \"layer2.0.conv3.weight\", \"layer2.0.bn3.weight\", \"layer2.0.bn3.bias\", \"layer2.0.bn3.running_mean\", \"layer2.0.bn3.running_var\", \"layer2.1.conv3.weight\", \"layer2.1.bn3.weight\", \"layer2.1.bn3.bias\", \"layer2.1.bn3.running_mean\", \"layer2.1.bn3.running_var\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.bn1.bias\", \"layer2.2.bn1.running_mean\", \"layer2.2.bn1.running_var\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.bn2.bias\", \"layer2.2.bn2.running_mean\", \"layer2.2.bn2.running_var\", \"layer2.2.conv3.weight\", \"layer2.2.bn3.weight\", \"layer2.2.bn3.bias\", \"layer2.2.bn3.running_mean\", \"layer2.2.bn3.running_var\", \"layer2.3.conv1.weight\", \"layer2.3.bn1.weight\", \"layer2.3.bn1.bias\", \"layer2.3.bn1.running_mean\", \"layer2.3.bn1.running_var\", \"layer2.3.conv2.weight\", \"layer2.3.bn2.weight\", \"layer2.3.bn2.bias\", \"layer2.3.bn2.running_mean\", \"layer2.3.bn2.running_var\", \"layer2.3.conv3.weight\", \"layer2.3.bn3.weight\", \"layer2.3.bn3.bias\", \"layer2.3.bn3.running_mean\", \"layer2.3.bn3.running_var\", \"layer3.0.conv3.weight\", \"layer3.0.bn3.weight\", \"layer3.0.bn3.bias\", \"layer3.0.bn3.running_mean\", \"layer3.0.bn3.running_var\", \"layer3.1.conv3.weight\", \"layer3.1.bn3.weight\", \"layer3.1.bn3.bias\", \"layer3.1.bn3.running_mean\", \"layer3.1.bn3.running_var\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.bn1.bias\", \"layer3.2.bn1.running_mean\", \"layer3.2.bn1.running_var\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.bn2.bias\", \"layer3.2.bn2.running_mean\", \"layer3.2.bn2.running_var\", \"layer3.2.conv3.weight\", \"layer3.2.bn3.weight\", \"layer3.2.bn3.bias\", \"layer3.2.bn3.running_mean\", \"layer3.2.bn3.running_var\", \"layer3.3.conv1.weight\", \"layer3.3.bn1.weight\", \"layer3.3.bn1.bias\", \"layer3.3.bn1.running_mean\", \"layer3.3.bn1.running_var\", \"layer3.3.conv2.weight\", \"layer3.3.bn2.weight\", \"layer3.3.bn2.bias\", \"layer3.3.bn2.running_mean\", \"layer3.3.bn2.running_var\", \"layer3.3.conv3.weight\", \"layer3.3.bn3.weight\", \"layer3.3.bn3.bias\", \"layer3.3.bn3.running_mean\", \"layer3.3.bn3.running_var\", \"layer3.4.conv1.weight\", \"layer3.4.bn1.weight\", \"layer3.4.bn1.bias\", \"layer3.4.bn1.running_mean\", \"layer3.4.bn1.running_var\", \"layer3.4.conv2.weight\", \"layer3.4.bn2.weight\", \"layer3.4.bn2.bias\", \"layer3.4.bn2.running_mean\", \"layer3.4.bn2.running_var\", \"layer3.4.conv3.weight\", \"layer3.4.bn3.weight\", \"layer3.4.bn3.bias\", \"layer3.4.bn3.running_mean\", \"layer3.4.bn3.running_var\", \"layer3.5.conv1.weight\", \"layer3.5.bn1.weight\", \"layer3.5.bn1.bias\", \"layer3.5.bn1.running_mean\", \"layer3.5.bn1.running_var\", \"layer3.5.conv2.weight\", \"layer3.5.bn2.weight\", \"layer3.5.bn2.bias\", \"layer3.5.bn2.running_mean\", \"layer3.5.bn2.running_var\", \"layer3.5.conv3.weight\", \"layer3.5.bn3.weight\", \"layer3.5.bn3.bias\", \"layer3.5.bn3.running_mean\", \"layer3.5.bn3.running_var\", \"layer4.0.conv3.weight\", \"layer4.0.bn3.weight\", \"layer4.0.bn3.bias\", \"layer4.0.bn3.running_mean\", \"layer4.0.bn3.running_var\", \"layer4.1.conv3.weight\", \"layer4.1.bn3.weight\", \"layer4.1.bn3.bias\", \"layer4.1.bn3.running_mean\", \"layer4.1.bn3.running_var\", \"layer4.2.conv1.weight\", \"layer4.2.bn1.weight\", \"layer4.2.bn1.bias\", \"layer4.2.bn1.running_mean\", \"layer4.2.bn1.running_var\", \"layer4.2.conv2.weight\", \"layer4.2.bn2.weight\", \"layer4.2.bn2.bias\", \"layer4.2.bn2.running_mean\", \"layer4.2.bn2.running_var\", \"layer4.2.conv3.weight\", \"layer4.2.bn3.weight\", \"layer4.2.bn3.bias\", \"layer4.2.bn3.running_mean\", \"layer4.2.bn3.running_var\". \n\tsize mismatch for layer1.0.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n\tsize mismatch for layer1.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for layer2.0.conv1.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for layer2.0.downsample.0.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for layer2.0.downsample.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer2.0.downsample.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer2.0.downsample.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer2.0.downsample.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer2.1.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for layer3.0.conv1.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1]).\n\tsize mismatch for layer3.0.downsample.0.weight: copying a param with shape torch.Size([256, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 512, 1, 1]).\n\tsize mismatch for layer3.0.downsample.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layer3.0.downsample.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layer3.0.downsample.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layer3.0.downsample.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layer3.1.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1024, 1, 1]).\n\tsize mismatch for layer4.0.conv1.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 1, 1]).\n\tsize mismatch for layer4.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for layer4.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for layer4.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for layer4.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for layer4.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for layer4.1.conv1.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 2048, 1, 1]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([1000, 512]) from checkpoint, the shape in current model is torch.Size([1000, 2048]).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m net \u001B[38;5;241m=\u001B[39m resnet50()\n\u001B[0;32m      6\u001B[0m model_keys \u001B[38;5;241m=\u001B[39m net\u001B[38;5;241m.\u001B[39mstate_dict()\u001B[38;5;241m.\u001B[39mkeys()\n\u001B[1;32m----> 7\u001B[0m \u001B[43mnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\IDE\\Miniconda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1604\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[1;34m(self, state_dict, strict)\u001B[0m\n\u001B[0;32m   1599\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[0;32m   1600\u001B[0m             \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   1601\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(k) \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[0;32m   1603\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 1604\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   1605\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)))\n\u001B[0;32m   1606\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"layer1.0.conv3.weight\", \"layer1.0.bn3.weight\", \"layer1.0.bn3.bias\", \"layer1.0.bn3.running_mean\", \"layer1.0.bn3.running_var\", \"layer1.0.downsample.0.weight\", \"layer1.0.downsample.1.weight\", \"layer1.0.downsample.1.bias\", \"layer1.0.downsample.1.running_mean\", \"layer1.0.downsample.1.running_var\", \"layer1.1.conv3.weight\", \"layer1.1.bn3.weight\", \"layer1.1.bn3.bias\", \"layer1.1.bn3.running_mean\", \"layer1.1.bn3.running_var\", \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.bn2.bias\", \"layer1.2.bn2.running_mean\", \"layer1.2.bn2.running_var\", \"layer1.2.conv3.weight\", \"layer1.2.bn3.weight\", \"layer1.2.bn3.bias\", \"layer1.2.bn3.running_mean\", \"layer1.2.bn3.running_var\", \"layer2.0.conv3.weight\", \"layer2.0.bn3.weight\", \"layer2.0.bn3.bias\", \"layer2.0.bn3.running_mean\", \"layer2.0.bn3.running_var\", \"layer2.1.conv3.weight\", \"layer2.1.bn3.weight\", \"layer2.1.bn3.bias\", \"layer2.1.bn3.running_mean\", \"layer2.1.bn3.running_var\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.bn1.bias\", \"layer2.2.bn1.running_mean\", \"layer2.2.bn1.running_var\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.bn2.bias\", \"layer2.2.bn2.running_mean\", \"layer2.2.bn2.running_var\", \"layer2.2.conv3.weight\", \"layer2.2.bn3.weight\", \"layer2.2.bn3.bias\", \"layer2.2.bn3.running_mean\", \"layer2.2.bn3.running_var\", \"layer2.3.conv1.weight\", \"layer2.3.bn1.weight\", \"layer2.3.bn1.bias\", \"layer2.3.bn1.running_mean\", \"layer2.3.bn1.running_var\", \"layer2.3.conv2.weight\", \"layer2.3.bn2.weight\", \"layer2.3.bn2.bias\", \"layer2.3.bn2.running_mean\", \"layer2.3.bn2.running_var\", \"layer2.3.conv3.weight\", \"layer2.3.bn3.weight\", \"layer2.3.bn3.bias\", \"layer2.3.bn3.running_mean\", \"layer2.3.bn3.running_var\", \"layer3.0.conv3.weight\", \"layer3.0.bn3.weight\", \"layer3.0.bn3.bias\", \"layer3.0.bn3.running_mean\", \"layer3.0.bn3.running_var\", \"layer3.1.conv3.weight\", \"layer3.1.bn3.weight\", \"layer3.1.bn3.bias\", \"layer3.1.bn3.running_mean\", \"layer3.1.bn3.running_var\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.bn1.bias\", \"layer3.2.bn1.running_mean\", \"layer3.2.bn1.running_var\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.bn2.bias\", \"layer3.2.bn2.running_mean\", \"layer3.2.bn2.running_var\", \"layer3.2.conv3.weight\", \"layer3.2.bn3.weight\", \"layer3.2.bn3.bias\", \"layer3.2.bn3.running_mean\", \"layer3.2.bn3.running_var\", \"layer3.3.conv1.weight\", \"layer3.3.bn1.weight\", \"layer3.3.bn1.bias\", \"layer3.3.bn1.running_mean\", \"layer3.3.bn1.running_var\", \"layer3.3.conv2.weight\", \"layer3.3.bn2.weight\", \"layer3.3.bn2.bias\", \"layer3.3.bn2.running_mean\", \"layer3.3.bn2.running_var\", \"layer3.3.conv3.weight\", \"layer3.3.bn3.weight\", \"layer3.3.bn3.bias\", \"layer3.3.bn3.running_mean\", \"layer3.3.bn3.running_var\", \"layer3.4.conv1.weight\", \"layer3.4.bn1.weight\", \"layer3.4.bn1.bias\", \"layer3.4.bn1.running_mean\", \"layer3.4.bn1.running_var\", \"layer3.4.conv2.weight\", \"layer3.4.bn2.weight\", \"layer3.4.bn2.bias\", \"layer3.4.bn2.running_mean\", \"layer3.4.bn2.running_var\", \"layer3.4.conv3.weight\", \"layer3.4.bn3.weight\", \"layer3.4.bn3.bias\", \"layer3.4.bn3.running_mean\", \"layer3.4.bn3.running_var\", \"layer3.5.conv1.weight\", \"layer3.5.bn1.weight\", \"layer3.5.bn1.bias\", \"layer3.5.bn1.running_mean\", \"layer3.5.bn1.running_var\", \"layer3.5.conv2.weight\", \"layer3.5.bn2.weight\", \"layer3.5.bn2.bias\", \"layer3.5.bn2.running_mean\", \"layer3.5.bn2.running_var\", \"layer3.5.conv3.weight\", \"layer3.5.bn3.weight\", \"layer3.5.bn3.bias\", \"layer3.5.bn3.running_mean\", \"layer3.5.bn3.running_var\", \"layer4.0.conv3.weight\", \"layer4.0.bn3.weight\", \"layer4.0.bn3.bias\", \"layer4.0.bn3.running_mean\", \"layer4.0.bn3.running_var\", \"layer4.1.conv3.weight\", \"layer4.1.bn3.weight\", \"layer4.1.bn3.bias\", \"layer4.1.bn3.running_mean\", \"layer4.1.bn3.running_var\", \"layer4.2.conv1.weight\", \"layer4.2.bn1.weight\", \"layer4.2.bn1.bias\", \"layer4.2.bn1.running_mean\", \"layer4.2.bn1.running_var\", \"layer4.2.conv2.weight\", \"layer4.2.bn2.weight\", \"layer4.2.bn2.bias\", \"layer4.2.bn2.running_mean\", \"layer4.2.bn2.running_var\", \"layer4.2.conv3.weight\", \"layer4.2.bn3.weight\", \"layer4.2.bn3.bias\", \"layer4.2.bn3.running_mean\", \"layer4.2.bn3.running_var\". \n\tsize mismatch for layer1.0.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n\tsize mismatch for layer1.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for layer2.0.conv1.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for layer2.0.downsample.0.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for layer2.0.downsample.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer2.0.downsample.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer2.0.downsample.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer2.0.downsample.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer2.1.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for layer3.0.conv1.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1]).\n\tsize mismatch for layer3.0.downsample.0.weight: copying a param with shape torch.Size([256, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 512, 1, 1]).\n\tsize mismatch for layer3.0.downsample.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layer3.0.downsample.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layer3.0.downsample.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layer3.0.downsample.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layer3.1.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1024, 1, 1]).\n\tsize mismatch for layer4.0.conv1.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 1, 1]).\n\tsize mismatch for layer4.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for layer4.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for layer4.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for layer4.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for layer4.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for layer4.1.conv1.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 2048, 1, 1]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([1000, 512]) from checkpoint, the shape in current model is torch.Size([1000, 2048])."
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T08:53:06.274402Z",
     "start_time": "2024-05-29T08:53:06.016291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. 解决关键字不匹配问题\n",
    "model_weights_path = './ckpt/resnet18.pth'\n",
    "# 加载预训练权重\n",
    "ckpt = torch.load(model_weights_path)\n",
    "net = resnet50()\n",
    "# 得到模型参数\n",
    "model_dict = net.state_dict()\n",
    "# 判断预训练模型中网络的模块是否修改后的网络中也存在，并且shape相同，如果相同则取出\n",
    "pretrained_dict = {k:v for k, v in ckpt.items() if k in model_dict and (v.shape == model_dict[k].shape)}\n",
    "# 更新之后的model_dict\n",
    "model_dict.update(pretrained_dict)\n",
    "# 加载真正需要的state_dict\n",
    "net.load_state_dict(model_dict, strict=True)"
   ],
   "id": "62c7bdbf44377cb2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T07:27:44.434559Z",
     "start_time": "2024-06-09T07:27:44.163059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 测试ResNet-50\n",
    "from models.image_encoders.resnet import ResNet50Layer4Lower, ResNet50Layer4Upper\n",
    "net1 = ResNet50Layer4Lower(pretrained=True)\n",
    "net2 = ResNet50Layer4Upper(2048,512,pretrained=True,norm_scale=4)"
   ],
   "id": "1bf64ee2e485e42f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T07:27:53.649229Z",
     "start_time": "2024-06-09T07:27:53.639202Z"
    }
   },
   "cell_type": "code",
   "source": "print(net1)",
   "id": "fe3e679c827f4c07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50Layer4Lower(\n",
      "  (_model): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T07:29:55.156631Z",
     "start_time": "2024-06-09T07:29:55.143373Z"
    }
   },
   "cell_type": "code",
   "source": "print(net2)",
   "id": "d25824d626936a49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50Layer4Upper(\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "982dde439c617cfb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
